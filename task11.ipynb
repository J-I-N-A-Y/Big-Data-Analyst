  import pandas as pd
import dask.dataframe as dd
import numpy as np
from datetime import datetime, timedelta

# Step 1: Simulate large user activity logs
np.random.seed(42)
n_rows = 1_000_000  # 1 million rows

users = [f'user_{i}' for i in range(1, 101)]  # 100 users
pages = ['home', 'products', 'cart', 'checkout', 'profile', 'support']

start_time = datetime(2024, 1, 1)

data = {
    'user_id': np.random.choice(users, size=n_rows),
    'page': np.random.choice(pages, size=n_rows),
    'duration_seconds': np.random.exponential(scale=60, size=n_rows),  # average time ~60s
    'timestamp': [start_time + timedelta(seconds=int(i)) for i in np.random.randint(0, 10**6, n_rows)]
}

df = pd.DataFrame(data)

# Step 2: Convert to Dask DataFrame
ddf = dd.from_pandas(df, npartitions=10)

# Step 3: Total time spent per page
total_time_per_page = ddf.groupby('page')['duration_seconds'].sum().compute().sort_values(ascending=False)
print("Total Time Spent per Page:\n", total_time_per_page)

# Step 4: Average session duration per user
avg_duration_per_user = ddf.groupby('user_id')['duration_seconds'].mean().compute().sort_values(ascending=False)
print("\nAverage Session Duration per User:\n", avg_duration_per_user.head(10))

# Step 5: Most visited pages
page_counts = ddf['page'].value_counts().compute()
print("\nMost Visited Pages:\n", page_counts)

# Step 6: Save output (optional)
total_time_per_page.to_csv("total_time_per_page.csv")
avg_duration_per_user.to_csv("avg_duration_per_user.csv")
